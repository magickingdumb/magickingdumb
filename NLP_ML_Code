import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import pandas as pd 

# Load training data
train_df = pd.read_csv('text_data.csv')

# Create tokenizer to vectorize text
tokenizer = Tokenizer() 
tokenizer.fit_on_texts(train_df['text'])
vocab_size = len(tokenizer.word_index) + 1

# Convert text to sequences
X = tokenizer.texts_to_sequences(train_df['text'])
X = pad_sequences(X)

# Load labels
y = train_df['label'].values

# Build LSTM network
model = Sequential()
model.add(Embedding(vocab_size, 100, input_length=X.shape[1]))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', 
              optimizer='adam',
              metrics=['accuracy'])
              
# Train network
model.fit(X, y, epochs=5, batch_size=32)

# Evaluate on test data
test_sentences = [
  'This movie was outstanding',
  'The food was not very good'
]

test_seq = tokenizer.texts_to_sequences(test_sentences)
test_seq = pad_sequences(test_seq)

predictions = model.predict(test_seq)
print(predictions)
